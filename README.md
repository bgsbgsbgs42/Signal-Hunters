# Signal Hunters üîç

A real-time trading signal discovery platform that uses advanced NLP and machine learning to identify predictive relationships between social media sentiment and cryptocurrency price movements.

## üåü Features

- **Real-time Data Ingestion**: Streams from Reddit, Discord, Telegram, and market APIs
- **Advanced Topic Modeling**: BERTopic-powered semantic analysis of social discussions
- **Granger Causality Testing**: Statistical validation of predictive relationships
- **Anomaly Detection**: Multi-model approach using Isolation Forests and LSTM autoencoders
- **Walk-Forward Backtesting**: Rigorous hypothesis validation with performance metrics
- **Signal Scoring**: Quantitative ranking based on lag, correlation, and novelty
- **Interactive Dashboard**: Streamlit-based web interface for hypothesis testing

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Sources  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Kafka Streams   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   PostgreSQL    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ   TimescaleDB   ‚îÇ
‚îÇ ‚Ä¢ Reddit API    ‚îÇ    ‚îÇ ‚Ä¢ Message Queue  ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Discord Bot   ‚îÇ    ‚îÇ ‚Ä¢ Data Pipeline  ‚îÇ    ‚îÇ ‚Ä¢ Social Posts  ‚îÇ
‚îÇ ‚Ä¢ Telegram API  ‚îÇ    ‚îÇ ‚Ä¢ NLP Processing ‚îÇ    ‚îÇ ‚Ä¢ Market Data   ‚îÇ
‚îÇ ‚Ä¢ Market APIs   ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ ‚Ä¢ Topic Models  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ML Pipeline    ‚îÇ    ‚îÇ   Signal Engine  ‚îÇ    ‚îÇ   Dashboard     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Topic Models  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Granger Tests  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Streamlit UI  ‚îÇ
‚îÇ ‚Ä¢ Sentiment     ‚îÇ    ‚îÇ ‚Ä¢ Backtesting    ‚îÇ    ‚îÇ ‚Ä¢ Visualization ‚îÇ
‚îÇ ‚Ä¢ Anomaly Det.  ‚îÇ    ‚îÇ ‚Ä¢ Scoring        ‚îÇ    ‚îÇ ‚Ä¢ Hypothesis    ‚îÇ
‚îÇ ‚Ä¢ Forecasting   ‚îÇ    ‚îÇ ‚Ä¢ Leaderboard    ‚îÇ    ‚îÇ   Testing       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Prerequisites

### System Requirements
- Python 3.8+
- PostgreSQL 12+ with TimescaleDB extension
- Apache Kafka (optional for production)
- 8GB+ RAM recommended
- CUDA-compatible GPU (optional, for faster ML training)

### API Keys Required
- Reddit API credentials
- Discord bot token
- Telegram API credentials
- Exchange API keys (Binance recommended)

## üöÄ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/signal-hunters.git
cd signal-hunters

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Database Setup

```bash
# Install PostgreSQL and TimescaleDB
# Ubuntu/Debian:
sudo apt update
sudo apt install postgresql postgresql-contrib
sudo apt install timescaledb-postgresql-12

# macOS:
brew install postgresql timescaledb

# Start PostgreSQL
sudo systemctl start postgresql

# Create database
sudo -u postgres createdb signal_hunters
sudo -u postgres psql -d signal_hunters -c "CREATE EXTENSION timescaledb;"
```

### 3. Configuration

Create a `.env` file in the project root:

```env
# Database
DB_HOST=localhost
DB_NAME=signal_hunters
DB_USER=postgres
DB_PASSWORD=yourpassword

# Reddit API
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=SignalHunters/1.0

# Discord
DISCORD_TOKEN=your_discord_token

# Telegram
TELEGRAM_API_ID=your_api_id
TELEGRAM_API_HASH=your_api_hash

# Exchange APIs
BINANCE_API_KEY=your_binance_key
BINANCE_SECRET=your_binance_secret

# Kafka (optional)
KAFKA_BROKERS=localhost:9092
```

### 4. Initialize Database Schema

```bash
python setup_database.py
```

### 5. Start the MVP Dashboard

```bash
streamlit run MVP.py
```

The dashboard will be available at `http://localhost:8501`

## üìÅ Project Structure

```
signal-hunters/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ setup_database.py
‚îú‚îÄ‚îÄ MVP.py                          # Streamlit dashboard
‚îú‚îÄ‚îÄ Data Ingestion.py              # Real-time data collectors
‚îú‚îÄ‚îÄ Full Implementation.py         # Complete pipeline
‚îú‚îÄ‚îÄ Granger Causality.py          # Statistical testing
‚îú‚îÄ‚îÄ Scoring Engine.py             # Signal evaluation
‚îú‚îÄ‚îÄ Unified Real Time NLP pipeline.py  # Topic modeling
‚îú‚îÄ‚îÄ Anomaly Detection.py          # ML-based detection
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ settings.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processors.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ connectors.py
‚îÇ   ‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topics.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomalies.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ forecasting.py
‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ granger.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backtesting.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scoring.py
‚îÇ   ‚îî‚îÄ‚îÄ dashboard/
‚îÇ       ‚îú‚îÄ‚îÄ app.py
‚îÇ       ‚îú‚îÄ‚îÄ components.py
‚îÇ       ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_data.py
‚îÇ   ‚îú‚îÄ‚îÄ test_ml.py
‚îÇ   ‚îî‚îÄ‚îÄ test_analysis.py
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ installation.md
    ‚îú‚îÄ‚îÄ api_reference.md
    ‚îî‚îÄ‚îÄ user_guide.md
```

## üîß Implementation Phases

### Phase 1: MVP Dashboard (Start Here)
- [ ] Run `MVP.py` for basic hypothesis testing
- [ ] Test with sample data
- [ ] Verify database connectivity

### Phase 2: Data Pipeline
- [ ] Configure API credentials
- [ ] Start data ingestion (`Data Ingestion.py`)
- [ ] Monitor data flow in dashboard

### Phase 3: ML Pipeline
- [ ] Train topic models on historical data
- [ ] Implement real-time NLP processing
- [ ] Add anomaly detection

### Phase 4: Signal Engine
- [ ] Deploy Granger causality testing
- [ ] Implement backtesting engine
- [ ] Create signal scoring system

### Phase 5: Production
- [ ] Deploy with Docker
- [ ] Set up monitoring and alerts
- [ ] Scale with Kubernetes

## üìä Usage Examples

### Testing a Hypothesis

```python
from src.analysis.granger import GrangerCausalityAnalyzer

# Example: Does Bitcoin ETF discussion predict price movements?
analyzer = GrangerCausalityAnalyzer()
result = await analyzer.analyze_topic(
    coin="BTC",
    topic_freq=etf_discussion_frequency
)

print(f"Optimal lag: {result['optimal_lag']} hours")
print(f"P-value: {result['p_value']:.4f}")
```

### Running Backtest

```python
from src.analysis.backtesting import BacktestingEngine

hypothesis = {
    'condition': 'sentiment > 0.8 AND volume > 1.5x_avg',
    'action': 'long',
    'hold_period': 6  # hours
}

engine = BacktestingEngine()
results = engine.test_hypothesis(hypothesis, historical_data)
print(f"Sharpe ratio: {results['sharpe']:.2f}")
```

## üîç Monitoring

The dashboard provides real-time monitoring of:

- **Data Flow**: Message rates from each source
- **Topic Trends**: Emerging discussion themes
- **Signal Performance**: Live P&L tracking
- **Anomaly Alerts**: Unusual market/social patterns

## ‚ö†Ô∏è Important Notes

### Data Sources
- Ensure compliance with API terms of service
- Implement rate limiting to avoid API bans
- Consider data privacy and user consent

### Trading Risks
- This is for research and educational purposes
- Past performance doesn't guarantee future results
- Always validate signals before live trading
- Use proper risk management

### Performance
- Topic models require significant memory (4GB+ recommended)
- GPU acceleration improves training speed
- Consider distributed processing for large datasets

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- BERTopic for semantic topic modeling
- Statsmodels for Granger causality testing
- Streamlit for rapid dashboard development
- TimescaleDB for time-series optimization


---

**‚ö° Ready to discover the next alpha signal? Start with Phase 1 and let's build something amazing!**
